{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code for the kernel I submitted to Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import all of the libraries we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import randint\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import Imputer, OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\", index_col=0)\n",
    "test_X = pd.read_csv(\"data/test.csv\", index_col=0)\n",
    "train_X = train.drop(\"Survived\", axis=1).copy()\n",
    "train_y = train[\"Survived\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Make dataset transformers\n",
    "Since we are trying to work on our scikit-learn skills, we make a number of transformers to do our feature generator. These could be reused in other projects (though to be honest, some of them are too specific to be useful outside this project. I would (and will) generalize them in the future if their ideas are useful in other projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Returns a subset of columns of a DataFrame\"\"\"\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.loc[:, self.attribute_names]\n",
    "    \n",
    "    \n",
    "class CategoryFactorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Takes a single column DataFrame of type object (usually\n",
    "    strings) and converts it into integer categories.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        encoded = X.factorize()[0].reshape(-1, 1)\n",
    "        return encoded\n",
    "    \n",
    "    \n",
    "class GenerateTitles(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Adds a Title column to the Titanic DataFrame using the Name column\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Check if the titles have already been added\n",
    "        if \"Title\" in X.columns:\n",
    "            return X\n",
    "        X[\"Title\"] = X.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "        title_mapping = {\n",
    "            \"Mlle\": \"Miss\", \"Mme\": \"Mrs\", \"Ms\": \"Miss\", \"Capt\": \"Military\", \n",
    "            \"Col\": \"Military\", \"Major\": \"Military\",  \"Countess\": \"Nobility\", \n",
    "            \"Don\": \"Nobility\", \"Dr\": \"Medical\", \"Jonkheer\": \"Nobility\", \n",
    "            \"Lady\": \"Nobility\", \"Rev\": \"Religous\", \"Sir\": \"Nobility\",\n",
    "        }\n",
    "        for from_title, to_title in title_mapping.items():    \n",
    "            X.loc[X.Title == from_title, \"Title\"] = to_title\n",
    "        return X\n",
    "    \n",
    "    \n",
    "class SumColumns(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Creates a new column for the sum of\n",
    "    the values in a given set of columns\n",
    "    \"\"\"\n",
    "    def __init__(self, in_cols, out_col):\n",
    "        self.in_cols = in_cols\n",
    "        self.out_col = out_col\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X[self.out_col] = X.loc[:, self.in_cols].sum(axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define piplelines for feature generation\n",
    "Using our custom transformers and several of scikit-learn's built in transformers, we define methods to generate pipelines that will generate specific features. Some of the pipelines are generic enough to generate features of a certain family (turning string columns into one hot vectors) while others are specific to a column (generating one hot vectors for the titles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def family_size_pipeline(to_one_hots=False):\n",
    "    pieces = [\n",
    "        ('summer', SumColumns([\"SibSp\", \"Parch\"], \"Family_Size\")),\n",
    "        ('selector', DataFrameSelector([\"Family_Size\"]))\n",
    "    ]\n",
    "    if to_one_hots:\n",
    "        pieces.append(('one_hot', OneHotEncoder()))\n",
    "    return Pipeline(pieces)\n",
    "\n",
    "\n",
    "def numerics_pipeline(attribute_names, scaling=False):\n",
    "    pieces = [\n",
    "        ('selector', DataFrameSelector(attribute_names)),\n",
    "        ('imputer', Imputer(strategy=\"median\")),\n",
    "    ]\n",
    "    if scaling:\n",
    "        pieces.append(('std_scaler', StandardScaler()))\n",
    "    return Pipeline(pieces)\n",
    "\n",
    "\n",
    "def ordinal_category_pipeline(attribute_name):\n",
    "    return Pipeline([\n",
    "        ('selector', DataFrameSelector(attribute_name)),\n",
    "        ('one_hot', OneHotEncoder())\n",
    "    ])\n",
    "\n",
    "\n",
    "def str_category_pipeline(attribute_name):\n",
    "    return Pipeline([\n",
    "        ('selector', DataFrameSelector(attribute_name)),\n",
    "        ('factorize', CategoryFactorizer()),\n",
    "        ('imputer', Imputer(missing_values=-1, strategy=\"most_frequent\")),  # For Embarked\n",
    "        ('one_hot', OneHotEncoder())\n",
    "    ])\n",
    "\n",
    "\n",
    "def title_pipeline():\n",
    "    return Pipeline([\n",
    "        ('titler', GenerateTitles()),\n",
    "        ('selector', DataFrameSelector(\"Title\")),\n",
    "        ('factorize', CategoryFactorizer()),\n",
    "        ('one_hot', OneHotEncoder()),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define the full final pipeline\n",
    "We use scikit-learn's FeatureUnion to combine the outputs of our various pipelines into a single dataframe to feed into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"family_size_pipeline\", family_size_pipeline()),\n",
    "    (\"num_scale_pipeline\", numerics_pipeline([\"Fare\", \"Age\"], scaling=True)),\n",
    "    (\"ord_to_one_hot\", ordinal_category_pipeline([\"Pclass\"])),\n",
    "    (\"sex_pipeline\", str_category_pipeline(\"Sex\")),\n",
    "    (\"embarked_pipeline\", str_category_pipeline(\"Embarked\")),\n",
    "    (\"title_generation\", title_pipeline()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Train our model\n",
    "We will use a random forest classifier as it seems to be the default model people are using right now (and it had the best performance for this set of features). We perform a randomized search with cross validation over two of the hyperparameters for our model. We then select the best model as specified by the default scoring method (accuracy for random forest classifiers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_prep = final_pipeline.fit_transform(train_X)\n",
    "params = {\n",
    "    'max_depth': randint(150, 350),\n",
    "    'min_samples_split': randint(10, 25),\n",
    "}\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500)\n",
    "forest_search = RandomizedSearchCV(rnd_clf, param_distributions=params, n_iter=400, n_jobs=6)\n",
    "forest_search.fit(train_X_prep, train_y)\n",
    "final_model = forest_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Prepare our submission\n",
    "We transform the test data using our feature pipeline and then use our best model to predict survival. The predictions are then written out to a csv file in the format Kaggle expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_prep = final_pipeline.fit_transform(test_X)\n",
    "predictions = final_model.predict(test_X_prep)\n",
    "submission_df = pd.DataFrame(\n",
    "    data=predictions,\n",
    "    index=test_X.index,\n",
    "    columns=['Survived']\n",
    ")\n",
    "submission_df.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
